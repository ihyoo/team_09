# """
# Make VectorDB with huggingface
# """

# # !pip install langchain langchain-community langchain-openai
# # !pip install pymupdf
# # !pip install openai
# # !pip install chromadb

# import os
# import re
# from dotenv import load_dotenv
# from langchain.chat_models import ChatOpenAI
# from langchain.agents import Tool, initialize_agent
# from langchain.agents.agent_types import AgentType
# from langchain.prompts import PromptTemplate
# from langchain.chains import LLMChain, RetrievalQAWithSourcesChain
# from langchain.vectorstores import Chroma
# from langchain.embeddings import HuggingFaceEmbeddings
# from langchain.document_loaders import PyMuPDFLoader
# from langchain.text_splitter import RecursiveCharacterTextSplitter

# # âœ… ëª¨ë¸ ì´ˆê¸°í™”
# load_dotenv()
# openai.api_key = os.getenv("OPENAI_API_KEY2")

# news = NaverNewsAPI()
# llm = ChatOpenAI(temperature=0.3)


# # âœ… í›„ë³´ì ì •ë³´
# candidates = ["ì´ì¬ëª…", "ê¹€ë¬¸ìˆ˜", "ì´ì¤€ì„", "ê¶Œì˜êµ­", "ì†¡ì§„í˜¸"]
# # PDF_FOLDER = "/home/ihy/yih/univ/2501/big_nlp/team_proj/team_09/team_09/data"
# PDF_FOLDER = "../../data"

# file_paths = {
#     name: [f"{PDF_FOLDER}20250604_ëŒ€í•œë¯¼êµ­_{name}_ì„ ê±°ê³µì•½ì„œ.pdf"] * 2
#     for name in candidates
# }

# # âœ… ë¬¸ì„œ ë¡œë”©
# all_documents = []
# for name, paths in file_paths.items():
#     for path in paths:
#         loader = PyMuPDFLoader(path)
#         data = loader.load()
#         for d in data:
#             d.metadata["candidate"] = name
#             page = d.metadata.get("page", "?")
#             d.metadata["source"] = f"{os.path.basename(path)}:p{page}"
#         all_documents.extend(data)

# # âœ… ë¬¸ì„œ ë¶„í• 
# splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
#     chunk_size=1000, chunk_overlap=200, encoding_name='cl100k_base'
# )
# documents = splitter.split_documents(all_documents)

# # âœ… ì„ë² ë”© ë° ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
# embedding_model = HuggingFaceEmbeddings(
#     model_name='jhgan/ko-sbert-nli',
#     model_kwargs={'device': 'cpu'},
#     encode_kwargs={'normalize_embeddings': True}
# )
# vectorstore = Chroma.from_documents(documents, embedding=embedding_model, persist_directory="chroma_db")
# vectorstore.persist()

# # âœ… í›„ë³´ë³„ Retriever
# retrievers = {
#     c: vectorstore.as_retriever(search_kwargs={"k": 6, "filter": {"candidate": c}})
#     for c in candidates
# }

# # âœ… í”„ë¡¬í”„íŠ¸
# policy_prompt = PromptTemplate(
#     input_variables=["summaries", "question"],
#     template="""
# ì•„ë˜ ë¬¸ì„œëŠ” ëŒ€í†µë ¹ í›„ë³´ì˜ ê³µì•½ì´ë‹¤. ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒ ìš”ì†Œë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€í•˜ë¼:
# - ì •ì±…ì˜ ëª©ì 
# - êµ¬ì²´ì  ìˆ˜ë‹¨ (ì‹œì„¤, ì œë„, ë²•ì•ˆ ë“±)
# - ì‹¤í–‰ ëŒ€ìƒ ë˜ëŠ” ì§€ì—­
# - ë¬¸ì„œìƒ ë“±ì¥í•œ êµ¬ì²´ì ì¸ ë‹¨ì–´(ìš©ì–´)ë¥¼ ì‚¬ìš©
# - ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•  ê²ƒ

# ë¬¸ë§¥:
# {summaries}

# ì§ˆë¬¸:
# {question}

# ë‹µë³€:
# """
# )

# compare_prompt_5way = PromptTemplate(
#     input_variables=["topic", "comparisons"],
#     template="""
# ë‹¤ìŒì€ '{topic}'ì— ëŒ€í•œ ëŒ€í†µë ¹ í›„ë³´ ê³µì•½ ìš”ì•½ì´ë‹¤. ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ ìì„¸íˆ ë¹„êµí•˜ë¼:

# 1. ì •ì±…ì˜ ëª©ì  ë¹„êµ
# 2. êµ¬ì²´ì  ìˆ˜ë‹¨ ë¹„êµ
# 3. ì‹¤í–‰ ëŒ€ìƒ ë˜ëŠ” ì§€ì—­ ë¹„êµ
# 4. ë¬¸ì„œìƒ ë“±ì¥í•œ êµ¬ì²´ì  ìš©ì–´ ë¹„êµ

# ì•„ë˜ í˜•ì‹ì„ ìœ ì§€í•˜ê³  ë¬¸ì¥ì„ ìš”ì•½í•˜ì§€ ë§ˆì‹œì˜¤. ë°˜ë“œì‹œ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ìƒì„¸íˆ ì‘ì„±í•˜ê³ , í›„ë³´ë³„ ì°¨ì´ì ì„ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œí•˜ì‹œì˜¤.

# í›„ë³´ë³„ ê³µì•½:
# {comparisons}

# ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì‹œì˜¤.
# """
# )

# translation_prompt = PromptTemplate(
#     input_variables=["english_text"],
#     template="ë‹¤ìŒ ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì‹­ì‹œì˜¤:\n\n{english_text}\n\në²ˆì—­:"
# )

# candidate_detect_prompt = PromptTemplate(
#     input_variables=["question"],
#     template="ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ ì–¸ê¸‰ëœ ëŒ€í†µë ¹ í›„ë³´ì˜ ì´ë¦„ì„ ì •í™•íˆ ì¶”ì¶œí•˜ì‹œì˜¤. ë‘ ëª… ì´ìƒì¼ ê²½ìš° ëª¨ë‘ ì¶œë ¥í•˜ì‹œì˜¤.\n\nì§ˆë¬¸: {question}\ní›„ë³´ ì´ë¦„:"
# )

# # âœ… ì²´ì¸ êµ¬ì„±
# qa_chains = {
#     name: RetrievalQAWithSourcesChain.from_chain_type(
#         llm=llm,
#         retriever=retrievers[name],
#         chain_type="stuff",
#         chain_type_kwargs={"prompt": policy_prompt},
#         return_source_documents=True
#     ) for name in candidates
# }
# compare_chain_5way = LLMChain(llm=llm, prompt=compare_prompt_5way)
# translation_chain = LLMChain(llm=llm, prompt=translation_prompt)
# candidate_chain = LLMChain(llm=llm, prompt=candidate_detect_prompt)

# # âœ… ìœ í‹¸ í•¨ìˆ˜
# def is_empty_or_irrelevant(answer: str) -> bool:
#     patterns = ["ê´€ë ¨.*ì—†", "ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤", "í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤", "ë“±ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤", "ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ë¬¸ì„œì—.*ì—†"]
#     return not answer.strip() or any(re.search(p, answer) for p in patterns)

# def is_english(text: str, threshold: float = 0.6) -> bool:
#     english_chars = re.findall(r'[a-zA-Z]', text)
#     total_chars = re.findall(r'\S', text)
#     return bool(total_chars) and len(english_chars) / len(total_chars) >= threshold

# def translate_if_needed(text: str) -> str:
#     if is_english(text):
#         result = translation_chain.invoke({"english_text": text})
#         return result["text"] if isinstance(result, dict) else result
#     return text

# # âœ… ì™¸ë¶€ ë‰´ìŠ¤ ìš”ì•½ fallback (news_final_summaryëŠ” ì™¸ë¶€ì—ì„œ ì •ì˜)
# def fallback_policy_response(candidate: str, keyword: str) -> str:
#     try:
#         summary_result = news_final_summary(candidate, keyword)
#         return summary_result or f"{candidate} í›„ë³´ì˜ '{keyword}' ê´€ë ¨ ì •ì±… ì •ë³´ë¥¼ ì™¸ë¶€ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
#     except Exception as e:
#         return f"{candidate} í›„ë³´ì˜ '{keyword}' ê´€ë ¨ ì™¸ë¶€ ë‰´ìŠ¤ ìš”ì•½ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# def format_candidate_policy(candidate: str, answer: str, is_external: bool) -> str:
#     source = "ğŸ“° ì™¸ë¶€ ë‰´ìŠ¤ ìš”ì•½ ê¸°ë°˜:" if is_external else "ğŸ“„ PDF ê¸°ë°˜ ê³µì•½:"
#     return f"[{candidate} í›„ë³´]\n{source}\n{answer.strip()}"

# def format_final_comparison(topic: str, comparisons: list[str]) -> str:
#     return f"""âœ… '{topic}'ì— ëŒ€í•œ í›„ë³´ë³„ ê³µì•½ ë¹„êµ ë¶„ì„ ê²°ê³¼\n\n{chr(10).join(comparisons)}\n\nâ€» ì¼ë¶€ í›„ë³´ì˜ ê²½ìš° PDFì— í•´ë‹¹ ì •ì±…ì´ ì—†ì–´ ì™¸ë¶€ ë‰´ìŠ¤ ìš”ì•½ ì •ë³´ê°€ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤."""

# # âœ… ë‹¨ì¼ í›„ë³´ ì§ˆì˜
# def run_candidate_policy_qa(input):
#     result = candidate_chain.invoke({"question": input})["text"]
#     target = next((c for c in candidates if c in result), None)
#     if not target:
#         return "í›„ë³´ ì´ë¦„ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

#     try:
#         answer = qa_chains[target].invoke({"question": input})["answer"]
#         is_external = is_empty_or_irrelevant(answer)
#         if is_external:
#             answer = fallback_policy_response(target, input)
#     except Exception:
#         is_external = True
#         answer = fallback_policy_response(target, input)

#     answer = translate_if_needed(answer)
#     return f"Final Answer:\n{format_candidate_policy(target, answer, is_external)}"

# # âœ… ë‹¤ì ë¹„êµ
# def run_policy_compare_all(input):
#     if "," in input:
#         split = [c.strip() for c in input.split(",")]
#         involved = [c for c in split if c in candidates]
#         keyword = next((k for k in split if k not in candidates), input)
#     else:
#         involved = candidates
#         keyword = input.strip()

#     comparisons = []
#     for cand in involved:
#         try:
#             raw_answer = qa_chains[cand].invoke({"question": keyword})["answer"]
#             is_external = is_empty_or_irrelevant(raw_answer)
#             answer = fallback_policy_response(cand, keyword) if is_external else raw_answer
#         except Exception:
#             is_external = True
#             answer = fallback_policy_response(cand, keyword)

#         answer = translate_if_needed(answer)
#         comparisons.append(format_candidate_policy(cand, answer, is_external))

#     result = compare_chain_5way.invoke({
#         "topic": keyword,
#         "comparisons": "\n\n".join(comparisons)
#     })["text"]
#     result = translate_if_needed(result)

#     return f"{format_final_comparison(keyword, comparisons)}\n\nâœ… ìµœì¢… ë¹„êµ ë¶„ì„\n{result}"

# # âœ… íˆ´ & ì—ì´ì „íŠ¸ ì„¤ì •
# react_tools = [
#     Tool(name="CandidatePolicyQA", func=run_candidate_policy_qa, description="ì§ˆë¬¸ì—ì„œ í›„ë³´ë¥¼ ì‹ë³„í•˜ê³  í•´ë‹¹ ê³µì•½ì„ ê²€ìƒ‰í•¨."),
#     Tool(name="ComparePolicies", func=run_policy_compare_all, description="ëª¨ë“  í›„ë³´ì˜ ê³µì•½ì„ ë¹„êµí•˜ê³  ì •ì±…ì´ ì—†ìœ¼ë©´ ì™¸ë¶€ í•¨ìˆ˜ í˜¸ì¶œë¡œ ë³´ì™„í•¨.")
# ]

# agent = initialize_agent(
#     tools=react_tools,
#     llm=llm,
#     agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
#     verbose=True,
#     agent_kwargs={
#         "system_message": (
#             "ë‹¹ì‹ ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‚¬ê³ í•˜ê³  ì‘ë‹µí•˜ëŠ” ì •ì±… ë¶„ì„ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. "
#             "Thought, Action, Observation, Final Answer í˜•ì‹ì„ ì‚¬ìš©í•˜ë©°, ëª¨ë“  ì¶œë ¥ì€ í•œêµ­ì–´ì—¬ì•¼ í•©ë‹ˆë‹¤. "
#             "**Final AnswerëŠ” ë°˜ë“œì‹œ ì •ì±… ë¹„êµì˜ ì„¸ë¶€ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ë¬¸ë‹¨ í˜•íƒœë¡œ ìì„¸íˆ ì‘ì„±í•˜ì‹­ì‹œì˜¤.**"
#         )
#     },
#     handle_parsing_errors=True
# )



import os
import re
from dotenv import load_dotenv
from langchain.chat_models import ChatOpenAI
from langchain.agents import Tool, initialize_agent
from langchain.agents.agent_types import AgentType
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain, RetrievalQAWithSourcesChain
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings

# âœ… í™˜ê²½ ì„¤ì • ë° ëª¨ë¸ ë¡œë”©
load_dotenv()
llm = ChatOpenAI(temperature=0.3)
embedding_model = HuggingFaceEmbeddings(
    model_name='jhgan/ko-sbert-nli',
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)
vectorstore = Chroma(persist_directory="chroma_db", embedding_function=embedding_model)

# âœ… í›„ë³´ì ëª©ë¡ ë° Retriever
candidates = ["ì´ì¬ëª…", "ê¹€ë¬¸ìˆ˜", "ì´ì¤€ì„", "ê¶Œì˜êµ­", "ì†¡ì§„í˜¸"]
retrievers = {
    c: vectorstore.as_retriever(search_kwargs={"k": 6, "filter": {"candidate": c}})
    for c in candidates
}

# âœ… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
policy_prompt = PromptTemplate(
    input_variables=["summaries", "question"],
    template="""..."""  # ê¸°ì¡´ ì •ì±… ìš”ì•½ í”„ë¡¬í”„íŠ¸ ê·¸ëŒ€ë¡œ ì‚½ì…
)
compare_prompt_5way = PromptTemplate(
    input_variables=["topic", "comparisons"],
    template="""..."""  # ê¸°ì¡´ ë¹„êµ í”„ë¡¬í”„íŠ¸ ê·¸ëŒ€ë¡œ ì‚½ì…
)
translation_prompt = PromptTemplate(
    input_variables=["english_text"],
    template="ë‹¤ìŒ ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ½ê³  ì •í™•í•œ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì‹­ì‹œì˜¤:\n\n{english_text}\n\në²ˆì—­:"
)
candidate_detect_prompt = PromptTemplate(
    input_variables=["question"],
    template="ë‹¤ìŒ ì§ˆë¬¸ì—ì„œ ì–¸ê¸‰ëœ ëŒ€í†µë ¹ í›„ë³´ì˜ ì´ë¦„ì„ ì •í™•íˆ ì¶”ì¶œí•˜ì‹œì˜¤. ë‘ ëª… ì´ìƒì¼ ê²½ìš° ëª¨ë‘ ì¶œë ¥í•˜ì‹œì˜¤.\n\nì§ˆë¬¸: {question}\ní›„ë³´ ì´ë¦„:"
)

# âœ… ì²´ì¸ êµ¬ì„±
qa_chains = {
    name: RetrievalQAWithSourcesChain.from_chain_type(
        llm=llm,
        retriever=retrievers[name],
        chain_type="stuff",
        chain_type_kwargs={"prompt": policy_prompt},
        return_source_documents=True
    ) for name in candidates
}
compare_chain_5way = LLMChain(llm=llm, prompt=compare_prompt_5way)
translation_chain = LLMChain(llm=llm, prompt=translation_prompt)
candidate_chain = LLMChain(llm=llm, prompt=candidate_detect_prompt)

# âœ… ìœ í‹¸ í•¨ìˆ˜ë“¤ (is_empty_or_irrelevant, translate_if_needed ë“±ì€ ê·¸ëŒ€ë¡œ ìœ ì§€)

# âœ… ì§ˆì˜ í•¨ìˆ˜ (run_candidate_policy_qa, run_policy_compare_all ë“±ì€ ê·¸ëŒ€ë¡œ ìœ ì§€)

# âœ… ReAct Agent ì„¤ì •
react_tools = [
    Tool(name="CandidatePolicyQA", func=run_candidate_policy_qa, description="ì§ˆë¬¸ì—ì„œ í›„ë³´ë¥¼ ì‹ë³„í•˜ê³  í•´ë‹¹ ê³µì•½ì„ ê²€ìƒ‰í•¨."),
    Tool(name="ComparePolicies", func=run_policy_compare_all, description="ëª¨ë“  í›„ë³´ì˜ ê³µì•½ì„ ë¹„êµí•˜ê³  ì •ì±…ì´ ì—†ìœ¼ë©´ ì™¸ë¶€ í•¨ìˆ˜ í˜¸ì¶œë¡œ ë³´ì™„í•¨.")
]

agent = initialize_agent(
    tools=react_tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
    agent_kwargs={
        "system_message": (
            "ë‹¹ì‹ ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‚¬ê³ í•˜ê³  ì‘ë‹µí•˜ëŠ” ì •ì±… ë¶„ì„ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. "
            "Thought, Action, Observation, Final Answer í˜•ì‹ì„ ì‚¬ìš©í•˜ë©°, ëª¨ë“  ì¶œë ¥ì€ í•œêµ­ì–´ì—¬ì•¼ í•©ë‹ˆë‹¤. "
            "**Final AnswerëŠ” ë°˜ë“œì‹œ ì •ì±… ë¹„êµì˜ ì„¸ë¶€ ë‚´ìš©ì„ í¬í•¨í•˜ì—¬ ë¬¸ë‹¨ í˜•íƒœë¡œ ìì„¸íˆ ì‘ì„±í•˜ì‹­ì‹œì˜¤.**"
        )
    },
    handle_parsing_errors=True
)

# âœ… ì˜ˆì‹œ ì‹¤í–‰
# agent.run("ì´ì¬ëª… í›„ë³´ì˜ ì²­ë…„ ì •ì±…ì€?")