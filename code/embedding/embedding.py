"""
Make VectorDB
"""

import os, time, json
import fitz  # PyMuPDF
from pathlib import Path
import chromadb
from openai import OpenAI
import unicodedata
from dotenv import load_dotenv

import sys
# í˜„ì¬ íŒŒì¼ì˜ ì ˆëŒ€ ê²½ë¡œì—ì„œ 5ë‹¨ê³„ ìƒìœ„ë¡œ ì´ë™ (team_projê¹Œì§€)
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

from log_utils.logger_config import get_logger
logger = get_logger("embeddings")


def normalize_korean(text):
    return unicodedata.normalize("NFC", text.strip().replace(" ", "").replace("\u200b", ""))

def clean_category(category_raw):
    # ë‹¤ì–‘í•œ êµ¬ë¶„ì ì œê±° ë° ì²« ë²ˆì§¸ë§Œ ì¶”ì¶œ
    for sep in ["Â·", "ã†", "/", "|", ";", "â€§"]:
        category_raw = category_raw.replace(sep, ",")
    parts = [p.strip() for p in category_raw.split(",") if p.strip()]
    first = parts[0] if parts else "ëª¨ë‘"
    return "ëª¨ë‘" if first == "ê¸°íƒ€" else first

def infer_party_from_question(question):
    party_synonyms = {
        "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹": ["ë”ë¯¼ì£¼", "ë¯¼ì£¼ë‹¹", "ë¯¼ì£¼"],
        "êµ­ë¯¼ì˜í˜": ["êµ­í˜", "êµ­ë¯¼ì˜ í˜", "êµ­í˜ë‹¹"],
        "ê°œí˜ì‹ ë‹¹": ["ê°œì‹ ", "ê°œí˜"],
        "ë¯¼ì£¼ë…¸ë™ë‹¹": ["ë¯¼ë…¸ë‹¹", "ë¯¼ë…¸"],
        "ë¬´ì†Œì†": ["ë¬´ì†Œì†", "ë…ë¦½", "ë¬´ë‹¹"]
    }
    for formal, synonyms in party_synonyms.items():
        for word in synonyms + [formal]:
            if word in question:
                return formal
    return "ëª¨ë‘"

# âœ… í…ìŠ¤íŠ¸ ì„ë² ë”©
def get_gpt_embedding(text, model="text-embedding-3-small"):
    try:
        response = client.embeddings.create(input=[text], model=model)
        return response.data[0].embedding
    except Exception as e:
        logger.info(f"Embedding error: {e}")
        return None

# âœ… GPT í˜¸ì¶œ
def get_completion(system_msg, user_prompt, model="gpt-4", retry=3):
    messages = [{"role": "system", "content": system_msg}, {"role": "user", "content": user_prompt}]
    for i in range(retry):
        try:
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.2,
                max_tokens=300
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.info(f"Retry {i+1} error: {e}")
            time.sleep(3)
    return None

# âœ… ë©”íƒ€ë°ì´í„° ë¶„ë¥˜
def classify_policy(text, candidate, party):
    prompt = f"""
ë‹¤ìŒ ì •ì±… ë‚´ìš©ì„ ë³´ê³  ì•„ë˜ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì¶”ì¶œí•´ì¤˜.
âš ï¸ ë°˜ë“œì‹œ JSON í˜•ì‹ì„ ì§€ì¼œì¤˜. ì´ì¤‘ ë”°ì˜´í‘œ(\"), ì¤„ë°”ê¿ˆ ì—†ìŒ, ì‘ë‹µ ì™¸ í…ìŠ¤íŠ¸ ì—†ì´.

\"{text[:1000]}\"

- category: ê²½ì œ, ë³µì§€, êµìœ¡, ì£¼ê±°, ë¶€ë™ì‚°, ë…¸ë™, ë³´ê±´, ì˜ë£Œ, ê³¼í•™ê¸°ìˆ , ì‚°ì—…, ì™¸êµ, ì•ˆë³´, í™˜ê²½, ê¸°í›„, ì²­ë…„, ì •ì±…, ì—¬ì„±, ê°€ì¡±, ì •ì¹˜, í–‰ì •, ê°œí˜, ë””ì§€í„¸, ë¯¸ë””ì–´, ê¸°íƒ€ ì¤‘ í•˜ë‚˜
- target_age: ì²­ë…„, ì¤‘ì¥ë…„, ë…¸ë…„, ëª¨ë‘
- target_gender: ë‚¨ì„±, ì—¬ì„±, ëª¨ë‘
- region: ì„œìš¸, ì „êµ­ ë“±
- party: {party} â† ì´ ê°’ ê·¸ëŒ€ë¡œ ë„£ì–´ì¤˜

í˜•ì‹:
{{"category":"...","target_age":"...","target_gender":"...","region":"...","party":"..."}}
"""
    result = get_completion("ë„ˆëŠ” ì •ì±… ë¶„ì„ ì „ë¬¸ê°€ì•¼.", prompt)
    try:
        return json.loads(result)
    except:
        return {"category": "ê¸°íƒ€", "target_age": "ëª¨ë‘", "target_gender": "ëª¨ë‘", "region": "ì „êµ­", "party": party}

# âœ… PDF ì²˜ë¦¬ í•¨ìˆ˜
def process_pdf_and_store(pdf_path):
    candidate_party = {
        "ì´ì¬ëª…": "ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹",
        "ê¹€ë¬¸ìˆ˜": "êµ­ë¯¼ì˜í˜",
        "ì´ì¤€ì„": "ê°œí˜ì‹ ë‹¹",
        "ê¶Œì˜êµ­": "ë¯¼ì£¼ë…¸ë™ë‹¹",
        "í™©êµì•ˆ": "ë¬´ì†Œì†",
        "ì†¡ì§„í˜¸": "ë¬´ì†Œì†"
    }
    doc = fitz.open(pdf_path)
    filename = Path(pdf_path).stem  # ex: 20250604_ëŒ€í•œë¯¼êµ­_ì´ì¬ëª…_ì„ ê±°ê³µì•½ì„œ
    parts = filename.split("_")     # â†’ ['20250604', 'ëŒ€í•œë¯¼êµ­', 'ì´ì¬ëª…', 'ì„ ê±°ê³µì•½ì„œ']
    parts = [x.strip().replace(" ","") for x in parts]

    if len(parts) < 4:
        logger.info(f"âš ï¸ íŒŒì¼ëª… êµ¬ì„±ì´ ì˜ëª»ë¨: {filename}")
        return

    file_type = parts[-1]  # ë§ˆì§€ë§‰ ìš”ì†Œ â†’ 'ì •ë‹¹ì •ì±…' or 'ì„ ê±°ê³µì•½ì„œ'
    # file_type = normalize_korean(file_type)
    logger.info(f"file_type - {file_type}")

    if file_type == "á„Œá…¥á†¼á„ƒá…¡á†¼á„Œá…¥á†¼á„á…¢á†¨":
        party = parts[2].strip().replace(" ","")
        # party = normalize_korean(party)
        candidate = ""
        policy_type = "party"
    elif file_type == "á„‰á…¥á†«á„€á…¥á„€á…©á†¼á„‹á…£á†¨á„‰á…¥":
        candidate = parts[2].strip().replace(" ","")
        candidate = normalize_korean(candidate)
        party = candidate_party.get(candidate, "ë¬´ì†Œì†")
        logger.info(f"candidate - {candidate}")
        logger.info(f"party - {party}")
        policy_type = "candidate"
    else:
        logger.info(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼ ìœ í˜•: {filename}")
        return

    for i, page in enumerate(doc):
        text = page.get_text().strip()
        if len(text) < 100:
            continue

        logger.info(f"ğŸ”¹ ì €ì¥ ì‹œë„: {candidate or party}_{i}")
        meta = classify_policy(text, candidate, party)
        meta["category"] = clean_category(meta.get("category", "ëª¨ë‘"))
        embedding = get_gpt_embedding(text)

        if embedding:
            log_meta_data = {
                "candidate": candidate,
                "party": party,
                "policy_type": policy_type,
                "category": meta["category"],
                "target_age": meta["target_age"],
                "target_gender": meta["target_gender"],
                "region": meta["region"]
            }
            logger.info(f"â†’ ì €ì¥ ë©”íƒ€: {log_meta_data}")
            
            collection.add(
                ids=[f"{candidate or party}_{i}"],
                documents=[text],
                embeddings=[embedding],
                metadatas=[{
                    "candidate": candidate,
                    "party": party,
                    "policy_type": policy_type,
                    "category": meta["category"],
                    "target_age": meta["target_age"],
                    "target_gender": meta["target_gender"],
                    "region": meta["region"]
                }]
            )
    doc.close()


if __name__ == "__main__":
    load_dotenv()
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")    
    
    PDF_FOLDER = "/home/ihy/yih/univ/2501/big_nlp/team_proj/team_09/data"
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    chroma_client = chromadb.PersistentClient(path="/home/ihy/yih/univ/2501/big_nlp/team_proj/team_09/vectorDB/chroma_policy_db")
    collection = chroma_client.get_or_create_collection(name="policy_collection")

    # âœ… ì „ì²´ PDF ì‹¤í–‰
    pdf_files = list(Path(PDF_FOLDER).glob("*.pdf"))
    print(f"pdf_files - {pdf_files}")
    for pdf in pdf_files:
        process_pdf_and_store(str(pdf))
